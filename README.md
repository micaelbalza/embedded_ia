Após a leitura do paper Widening Access to Applied Machine Learning with TinyML, evidencia-se o crescimento nas últimas décadas das aplicações que usam Inteligência artificial nos mais diversos setores como educação, segurança, saúde, veículos autônomos, assistência pessoal, dentre outros. Muitos desses nichos tem um apelo social oque os torna indispensáveis para melhorar a qualidade de vida de muitas pessoas. Se pensarmos nos modelos tradicionais de Machine Learning, onde grandes quantidades de dados e grandes infra estruturas são usadas para armazenar e processar os dados, temos uma barreira social pois este tipo de infraestrutura requer muito dinheiro e consome muita energia, o conceito de TinyML entra quando queremos democratizar/descentralizar os programas e hardwares de IA, colocando-os em sistemas embarcados, que normalmente são sistemas de baixo custo comparados a solução anterior e que possuem um baixo consumo de energia, algumas vantagens e desvantagens podem ser citadas entre TinyML e ML que são a baixa latência pois o algoritmo roda localmente não dependendo unicamente da rede para troca de mensagens com servidores, segurança pois exatamente essa troca de informação entre um dispositivo e o servidor com a ML pode ser interceptado/vazado e quando processado localmente com TinyML a informação permanece no dispositivo.

Percebeu-se também que pode-se usar TinyML para ensinar os conceitos de inteligência artificial, mostrando todas as etapas dos processos, tais como aquisição , tratamento dos dados, treinamento, inferência, dando uma ideia por completo do fluxo das etapas em um sistema em menor escala, que é exatamente igual as etapas que ocorrem em grandes sistemas, viabilizando assim o aprendizado, pois agora não é mais necessário grandes máquinas para aprender e aplicar os conceitos. Sistemas embarcados são extremamente importantes pois mesmo com sistemas simples, resolvem problemas onde o seu poder de processamento e consumo energético são mais adequados que grandes máquinas. Desta forma foi pensado em um curso, que foi ministrado entre 2020-2021 na universidade de Harvard exatamente com este tema, onde foi criado uma parceria a universidade e o Google para desenvolver o conteúdo, ferramentas e ministrar o curso, pensado nos mais variados tipos de pessoas, desde entusiastas em ML a doutores da área com interesse em TinyML como em diferentes níveis como ensino médio e superior.
A ideia foi de dividir o curso em etapas onde cada etapa pudesse ser pulada ou assistida fora da ordem de acordo com o nível/interesse do aluno, o primeiro curso por exemplo, tem os fundamentos de ML e conceitos de eletrônica e sistemas embarcados, que podem ser pulados caso o aluno já domina este assunto, o segundo refere-se mais a área de sistema de tempo real, pois esses dispositivos normalmente estão lidando com processamento de sinais e interrupções que são processadas, e por fim a etapa que mostra o fluxo de trabalho e experiência prática, visando preparar para o dia a dia na prática.
Outro fator importante que pode ser ressaltado é o fator social, que devemos ter com esse tipo de tecnologia, ter todo o cuidado necessário e saber lidar pois não são raros os casos em que a falta de cuidado nos dados ou no treinamento leva a um modelo que pode ferir a integridade moral de algo ou alguém, então usar TinyML como forma de democratizar o aprendizado, o cuidado e gerar benefícios sociais antes não atingidos por modelos tradicionais de ML.










Resumo: 
TinyMLOps: Operational Challenges for Widespread Edge AI Adoption

O paper traz uma abordagem sobre os desafios que são enfrentados ao implementar soluções em TimyML, principalmente comparando o poder de processamento destes dispositivos com o poder normalmente utilizado em aplicações de ML bem como vantagens deste tipo de abordagem como baixa latẽncia na aplicação e aumento da privacidade, tópicos que serão abordados mais adiante. O assunto é bem pertinente já que ultimamente houve um aumento no emprego de soluções em ML nas mais diversas áreas e aplicações, englobando a área de sistemas  embarcados como por exemplo soluções em smart home, assistentes virtuais e veículos autônomos.
A maior parte do trabalho de uma aplicação em TimyML se concentra em melhorar a eficiência do modelo em dispositivos com recursos limitados onde várias técnicas são utilizadas para reduzir o custo computacional, consumo de memória e consumo energético dos modelos. O foco do paper não é exatamente este, e sim mostrar os desafios encontrados na operação e produção destes sistemas onde poucos modelos saem da fase de testes e realmente entram em produção.
Pode-se separar os modelos de ML em dois grupos, o modelo centralizado é baseado em aplicações em nuvem, onde normalmente existe um modelo para todos os usuários e a arquitetura do sistema é a mais eficiente e atualizada para a aplicação, e o modelo descentralizado que é baseado em aplicações de borda, onde o modelo é implementado diretamente nos dispositivos do usuário final onde o tipo de hardware, poder computacional, qualidade da conexão, dentre outras configurações que variam e acabam por dificultar o trabalho de criação dos modelos pois é necessário definir necessidades técnicas que devem ser atingidas e otimizações realizadas para garantir o funcionamento da aplicação nestes dispositivos. 
Quatro pontos importantes são mencionados no texto comparando os dois tipos de modelos com relação às dificuldades e soluções que podem ser adotadas, são elas o gerenciamento de versões, observabilidade, pagamento pelo modelo de negócio e personalização do modelo e retreinamento. Seguindo a ordem é necessário conseguir dimensionar uma aplicação com uma certa precisão no modelo em um dispositivo com capacidade limitada, normalmente pega-se um modelo maior e faz as podas e ajustes necessários a fim de embarcá-lo, algumas operações podem geram modelos específicos e com isso se tem o aumento de versões no qual precisa ser gerenciado.
Com relação a observabilidade, o argumento de melhora da privacidade em sistemas implantados na borda é quebrado quando periodicamente são coletados e compartilhados dados com a central. Para contornar este problema e ainda sim obter um feedback analisando a performance, consumo energético, dentre outros dados sugere-se internamente guardar dados estatísticos básicos e compartilhar anonimamente possibilitando assim futuras otimizações no modelo. Os outros dois pontos referem-se respectivamente aos desafios em cobrar pelo treinamento e desenvolvimento em dispositivos de borda e posteriormente os desafios encontrados em gerenciar as atualizações dos vários modelos rodando em diversas arquiteturas diferentes, e que essa etapa é inevitável já que constantemente são gerados novos dados os modelos de ML são atualizados, o paper cita também a ideia de federed learning que é quando o usuário baixa o modelo atual e o atualiza com seus próprios dados, estas atualizações são então enviadas para a nuvem e compartilhadas com outros usuários que se beneficiam da atualização, mostrando também as vantagens, dificuldades e problemas neste tipo de abordagem.
Ao final do texto é frisado dois tipos de desafios que são encontrados no dia a dia em  projetos em TimyML, um deles refere-se às restrições em hardware dos dispositivos e em software para o desenvolvimento das aplicações citando os esforços no desenvolvimento de novos frameworks, e por fim os dicas sobre como proteger ou marcar o modelo em caso de roubo.
